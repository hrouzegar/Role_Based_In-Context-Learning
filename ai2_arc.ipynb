{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vtYnMkwNFMB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ki4ATFHOY3a",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"max_colwidth\", 100)\n",
        "pd.set_option(\"display.max_rows\",10)\n",
        "pd.set_option(\"display.max_columns\",5)\n",
        "DS= pd.read_pickle('balanced_ai2_arc_challenge.pkl');\n",
        "answer_data = DS\n",
        "answer_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7tKhS7B7PStr"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    api_key='your api key',\n",
        ")"
      ],
      "metadata": {
        "id": "t3qVp84Cbmsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose the desired model: so only one of the next two cells shold run"
      ],
      "metadata": {
        "id": "Cq3hbkUYbR-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(messages, model=\"gpt-4o\"):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "Yf35JJhnbNRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(messages, model=\"gpt-3.5-turbo-0125\"):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "t6vp9qTZbOrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Glnv5CoIQlOk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import time\n",
        "\n",
        "def connect_with_api(messages):\n",
        "    max_retries = 3\n",
        "    retries = 0\n",
        "\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            response = get_completion(messages)\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            retries += 1\n",
        "            print(f\"Retrying... (Attempt {retries} of {max_retries})\")\n",
        "            time.sleep(1)  # Wait for 1 second before retrying\n",
        "\n",
        "    print(\"Failed to connect with the API after multiple attempts.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o_6ngaONlWk"
      },
      "outputs": [],
      "source": [
        "test_samples_per_answer = 199\n",
        "train_samples_per_answer = 1\n",
        "test_df = pd.DataFrame()\n",
        "train_df = pd.DataFrame()\n",
        "for answer in answer_data['answer'].unique():\n",
        "    answer_df = answer_data[answer_data['answer'] == answer]\n",
        "    test_samples = answer_df.sample(test_samples_per_answer, random_state=1)\n",
        "    train_samples = answer_df.drop(test_samples.index).sample(train_samples_per_answer, random_state=1)\n",
        "\n",
        "    test_df = pd.concat([test_df, test_samples])\n",
        "    train_df = pd.concat([train_df, train_samples])\n",
        "\n",
        "# Reset the index of the resulting DataFrames\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "train_df = train_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RceCnC4XND-I"
      },
      "outputs": [],
      "source": [
        "def clean_response(response):\n",
        "    response = response.strip()\n",
        "    if 'A' in response:\n",
        "        return 'A'\n",
        "    elif 'B' in response:\n",
        "        return 'B'\n",
        "    elif 'C' in response:\n",
        "        return 'C'\n",
        "    elif 'D' in response:\n",
        "        return 'D'\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Experiment configurations\n",
        "def run_experiment(prompt_design, test_df, train_df, experiment_name):\n",
        "    test_results = []\n",
        "    for index, row in test_df.iterrows():\n",
        "        if prompt_design == \"zeroU\":\n",
        "            messages = [{\"role\": \"user\", \"content\": f\"\"\"Determine the answer of the example based on the provided question:\n",
        "             For the question provided, classify its answer as a single Capital letter (without other marks or words like 'answer:'), either \"A\", \"B\", \"C\", or \"D\".\n",
        "```{row['question']}```\"\"\" }]\n",
        "        elif prompt_design == \"zeroSU\":\n",
        "            messages = [{\"role\": \"system\", \"content\": f\"\"\"Determine the answer of the example based on the provided question.\n",
        "             For the question provided, classify its answer as a single Capital letter (without other marks or words like 'answer:'), either \"A\", \"B\", \"C\", or \"D\".\"\"\"},\n",
        "                        {\"role\": \"user\", \"content\": f\"\"\"```{row['question']}```\"\"\"}]\n",
        "        elif prompt_design == \"fewU\":\n",
        "            messages = [{\"role\": \"user\", \"content\": f\"\"\"Determine the answer of the example based on the provided question:\n",
        "             For the question provided, classify its answer as a single Capital letter (without other marks or words like 'answer:'),either \"A\", \"B\", \"C\", or \"D\".\n",
        "Examples:\n",
        "```{train_df.iloc[0]['question']}``` - {train_df.iloc[0]['answer']}\n",
        "```{train_df.iloc[1]['question']}``` - {train_df.iloc[1]['answer']}\n",
        "```{train_df.iloc[2]['question']}``` - {train_df.iloc[2]['answer']}\n",
        "...\n",
        "```{row['question']}```\"\"\" }]\n",
        "        elif prompt_design == \"fewSU\":\n",
        "            messages = [{\"role\": \"system\", \"content\": f\"\"\"Determine the answer of the example based on the provided question.\n",
        "             For the question provided, classify its answer as a single Capital letter (without other marks or words like 'answer:'), either \"A\", \"B\", \"C\", or \"D\".\"\"\"},\n",
        "                        {\"role\": \"user\", \"content\": f\"\"\"Examples:\n",
        "```{train_df.iloc[0]['question']}``` - {train_df.iloc[0]['answer']}\n",
        "```{train_df.iloc[1]['question']}``` - {train_df.iloc[1]['answer']}\n",
        "```{train_df.iloc[2]['question']}``` - {train_df.iloc[2]['answer']}\n",
        "...\n",
        "```{row['question']}```\"\"\"}]\n",
        "        elif prompt_design == \"fewSUA\":\n",
        "            messages = [{\"role\": \"system\", \"content\": f\"\"\"Determine the answer of the example based on the provided question.\n",
        "            For the question provided, classify its answer as a single Capital letter (without other marks or words like 'answer:'), either either \"A\", \"B\", \"C\", or \"D\".\"\"\"},\n",
        "                        {\"role\": \"user\", \"content\": train_df.iloc[0]['question']}, {\"role\": \"assistant\", \"content\": train_df.iloc[0]['answer']},\n",
        "                        {\"role\": \"user\", \"content\": train_df.iloc[1]['question']}, {\"role\": \"assistant\", \"content\": train_df.iloc[1]['answer']},\n",
        "                        {\"role\": \"user\", \"content\": train_df.iloc[2]['question']}, {\"role\": \"assistant\", \"content\": train_df.iloc[2]['answer']},\n",
        "                        {\"role\": \"user\", \"content\": f\"\"\"```{row['question']}```\"\"\"}]\n",
        "\n",
        "        prediction = connect_with_api(messages)\n",
        "        print(index, ' ', prediction)\n",
        "\n",
        "        test_results.append({'Index': index, 'question': row['question'], 'Predicted_answer': prediction, 'Actual_answer': row['answer']})\n",
        "\n",
        "    results_df = pd.DataFrame(test_results)\n",
        "    # Calculate structural accuracy before cleaning\n",
        "    valid_answer = ['A', 'B', 'C', 'D']\n",
        "    results_df['Is_Structurally_Valid'] = results_df['Predicted_answer'].apply(lambda x: x.strip().upper() in valid_answer)\n",
        "    structural_accuracy = results_df['Is_Structurally_Valid'].mean()\n",
        "\n",
        "    # Calculate F1 Score\n",
        "    results_df['Predicted_answer'] = results_df['Predicted_answer'].apply(clean_response)\n",
        "    results_df = results_df.dropna(subset=['Predicted_answer', 'Actual_answer'])\n",
        "    f1 = f1_score(results_df['Actual_answer'], results_df['Predicted_answer'], average='micro')\n",
        "\n",
        "    print(f\"Experiment: {experiment_name}\")\n",
        "    print(f\"The F1 Score for the predicted answer is: {f1}\")\n",
        "    print(f\"The Structural Accuracy is: {structural_accuracy}\")\n",
        "\n",
        "    results_df.to_csv(f\"/content/drive/MyDrive/Colab Notebooks/In_context_learning/{experiment_name}_results.csv\", index=False)\n",
        "    print(f\"Results for {experiment_name} saved to {experiment_name}_results.csv\")"
      ],
      "metadata": {
        "id": "Hys_Gjt3QIsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qnA12uxJZTyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the experiments\n",
        "run_experiment(\"zeroU\", test_df, train_df, \"Zero-shot User Prompt\")\n"
      ],
      "metadata": {
        "id": "arPcMkwhZVhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment(\"zeroSU\", test_df, train_df, \"Zero-shot System and User Prompt\")\n"
      ],
      "metadata": {
        "id": "ynrCPV3lZMLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment(\"fewU\", test_df, train_df, \"Few-shot User Prompt\")\n"
      ],
      "metadata": {
        "id": "HuTC5WcJZM3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment(\"fewSU\", test_df, train_df, \"Few-shot System and User Prompt\")\n"
      ],
      "metadata": {
        "id": "1HGruLaPZO7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment(\"fewSUA\", test_df, train_df, \"Few-shot System, User, and Assistant Prompt\")"
      ],
      "metadata": {
        "id": "FFEU44FwZQjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XsWclXKHZ4AM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}